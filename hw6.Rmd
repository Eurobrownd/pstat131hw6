---
title: "hw6"
author: "Simon Lee"
date: "2022-11-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message = FALSE}
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(corrr)
library(corrplot)
library(klaR)
library(glmnet)
library(MASS)
library(discrim)
library(poissonreg)
library(janitor)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(recipes)
tidymodels_prefer()
```

# q1
```{r}
poke_data <- read.csv("data/pokemon.csv")
poke_data <- poke_data %>% clean_names()
fpoke_data <- poke_data %>% filter((type_1 == "Bug" | type_1 == "Fire" | type_1 == "Grass" | type_1 == "Normal" |
                           type_1 == "Water" | type_1 == "Psychic"))
fpoke_data$type_1 <- as.factor(fpoke_data$type_1)
fpoke_data$legendary <- as.factor(fpoke_data$legendary)
fpoke_data$generation <- as.factor(fpoke_data$generation)

set.seed(558)
poke_split <- initial_split(fpoke_data, prop = 0.8, strata = type_1)
poke_train <- training(poke_split)
poke_test <- testing(poke_split)

poke_fold <- vfold_cv(data = poke_train, v=5, strata = type_1)
poke_fold

poke_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data= poke_train) %>% 
  step_dummy(legendary) %>% 
  step_dummy(generation) %>% 
  step_normalize(all_predictors())
```

# q2
```{r}
fpoke_data %>% select(is.numeric) %>% 
  cor() %>% 
  corrplot(type = "lower")
```

Looking at the correlation plot there seems to be no negative correlation between any of the variables. There does seem to be positive correlation between speed with attack and special attack. Special defense is correlated with defense. Which makes sense
as something with high defense should have high special defense and something with strong speed should be high attack since its offensive.

# q3
```{r, include= FALSE}
poke_tree_spec <- decision_tree(cost_complexity = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("rpart")

poke_tree_wf <- workflow() %>% 
  add_recipe(poke_recipe) %>% 
  add_model(poke_tree_spec)

param_grid_tree <- grid_regular(cost_complexity(range = c(-3,-1)), levels = 10)

tune_res_tree <- tune_grid(poke_tree_wf, resamples = poke_fold, grid = param_grid_tree, metrics = metric_set(roc_auc))
```

```{r}
autoplot(tune_res_tree)
```

As cpst_complexity increases the overall roc_auc goes down. It slightly increases a little bit before 0.010 and then the model
performs worse and worse until the end

# q4
```{r}
collect_metrics(tune_res_tree) %>%  arrange(desc(mean))
best_parameter_tree <- select_best(tune_res_tree, metric = "roc_auc")
best_parameter_tree
```

The best performing decision tree was 0.655, and it had a cost_complexity of 0.001

# q5
```{r}
poke_tree_final <- finalize_workflow(poke_tree_wf, best_parameter_tree)
poke_tree_final_fit <- fit(poke_tree_final, data = poke_train)
poke_tree_final_fit %>% extract_fit_engine() %>% rpart.plot(roundint = FALSE)
```

```{r}
poke_rf_spec <- rand_forest(mtry=tune(), trees=tune(), min_n=tune()) %>% 
  set_engine("randomForest", importance = TRUE) %>% 
  set_mode("classification")

poke_rf_wf <- workflow() %>% 
  add_recipe(poke_recipe) %>% 
  add_model(poke_rf_spec)
```

mtry is the number of randomly selcted variables each tree is given

trees represents the number of trees in the forest

min_n is the minimum # of data points in each node that are required for further splitting

```{r}
param_grid_rf <- grid_regular(mtry(range = c(1,8)),
                              trees(range = c(1,10)),
                              min_n(range = c(1,10)),
                              levels = 8)
param_grid_rf
```

Since we haev 9 predictors in our dataset, mtry should range from 1 to 8. Otherwise, we would be using predictors that
aren't available

# q6
```{r, include= FALSE}
tune_res_rf <- tune_grid(poke_rf_wf, resamples = poke_fold, grid = param_grid_rf, metrics = metric_set(roc_auc))
```

```{r}
autoplot(tune_res_rf)
```

Throughout all the models, it seems that the best performing models had 7, 8, or 10 trees. Generally increasing the number of
randomly selected predictors seems to increase the roc_auc score.

# q7
```{r}
collect_metrics(tune_res_rf) %>% arrange(desc(mean))
best_parameter_rf <- select_best(tune_res_rf, metric = "roc_auc")
best_parameter_rf
```

The best performing model was (mtry = 6, trees = 10, and min_n = 1) with a roc_auc score of 0.700

# q8
```{r}
poke_rf_final <- finalize_workflow(poke_rf_wf, best_parameter_rf)
poke_rf_final_fit <- fit(poke_rf_final, data = poke_train)
poke_rf_final_fit %>% extract_fit_engine() %>% vip()
```

The most important variables were special attack and hp. While the generation of the pokemon was not that useful at all.
This makes sense as which generation the pokemon came out doesn't have anything to do with its type while its hp stat
could explain whether it is a strong typing or a weak typing

# q9
```{r}
poke_boosted_spec <- boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

poke_boosted_wf <- workflow() %>% 
  add_recipe(poke_recipe) %>% 
  add_model(poke_boosted_spec)

param_grid_boosted <- grid_regular(trees(range = c(10, 2000)), levels = 10)

tune_res_boosted <- tune_grid(poke_boosted_wf, resamples = poke_fold, grid = param_grid_boosted, metrics = metric_set(roc_auc))
```

```{r}
autoplot(tune_res_boosted)
collect_metrics(tune_res_boosted) %>% arrange(desc(mean))
```

452 trees had the highest roc_auc score while the lowest score is around 10 trees.


```{r}
best_parameter_boosted <- select_best(tune_res_boosted, metric = "roc_auc")
best_parameter_boosted
```

# q10
```{r}
tree_auc <- collect_metrics(tune_res_tree) %>% arrange(desc(mean))
forest_auc <- collect_metrics(tune_res_rf) %>% arrange(desc(mean))
boosted_auc <- collect_metrics(tune_res_boosted) %>% arrange(desc(mean))
roc_aucs <- c(tree_auc$mean[1], forest_auc$mean[1], boosted_auc$mean[1])
roc_aucs
```
From the three, it seems that the boosted tree tree model with 10 trees performed best with 0.7122 roc_auc score

```{r}
poke_final <- finalize_workflow(poke_boosted_wf, best_parameter_boosted)
poke_final_fit <- fit(poke_final, data = poke_train)
testing_roc_auc <- augment(poke_final_fit, new_data = poke_test) %>% 
  roc_auc(truth = type_1, estimate= c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water))
testing_roc_auc
```

```{r}
roc_curves <- augment(poke_final_fit, new_data = poke_test) %>%
  roc_curve(truth = type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Psychic, .pred_Water)) %>% 
  autoplot()
roc_curves
```

```{r}
final_model_conf <- augment(poke_final_fit, new_data = poke_test) %>% 
  conf_mat(truth = type_1, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
final_model_conf
```

The model was best at predicting psychic water and normal types. Grass was the worst as it only got 2 correct And the model was okay at predicting bug and fire type.